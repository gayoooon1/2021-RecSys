{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading torch-1.10.1-cp37-cp37m-win_amd64.whl (226.6 MB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\owner\\anaconda3\\lib\\site-packages (from torch) (3.7.4.3)\n",
      "Installing collected packages: torch\n",
      "Successfully installed torch-1.10.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft vs Hard?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.from_numpy(np.array([[1,2,3],[10,5,1],[9,8,7]]).astype(np.float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 3])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_soft = F.gumbel_softmax(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.2031e-02, 4.6976e-02, 9.3099e-01],\n",
      "        [9.9402e-01, 5.9503e-03, 3.1650e-05],\n",
      "        [6.4901e-01, 2.9863e-01, 5.2354e-02]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(result_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They will be probability distributions that sum to 1 across dim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_hard = F.gumbel_softmax(test, hard=True) # one-hot으로 바뀌어 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(result_hard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tau (Temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tem_list = [0.1, 0.5, 1.0, 2.0, 10.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tau :  0.1\n",
      "tensor([[106., 244., 650.],\n",
      "        [990.,  10.,   0.],\n",
      "        [676., 233.,  91.]], dtype=torch.float64) \n",
      "\n",
      "tau :  0.5\n",
      "tensor([[ 85., 253., 662.],\n",
      "        [994.,   6.,   0.],\n",
      "        [684., 220.,  96.]], dtype=torch.float64) \n",
      "\n",
      "tau :  1.0\n",
      "tensor([[ 90., 244., 666.],\n",
      "        [990.,   9.,   1.],\n",
      "        [683., 234.,  83.]], dtype=torch.float64) \n",
      "\n",
      "tau :  2.0\n",
      "tensor([[ 99., 233., 668.],\n",
      "        [996.,   4.,   0.],\n",
      "        [652., 248., 100.]], dtype=torch.float64) \n",
      "\n",
      "tau :  10.0\n",
      "tensor([[ 99., 223., 678.],\n",
      "        [996.,   4.,   0.],\n",
      "        [663., 244.,  93.]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for tau in tem_list:\n",
    "    print('tau : ', tau)\n",
    "    sum_array = torch.from_numpy(np.zeros(result_hard.shape))\n",
    "    for n in range(1000):\n",
    "        result_hard = F.gumbel_softmax(test, hard=True, tau=tau)\n",
    "        sum_array += result_hard\n",
    "    print(sum_array, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 많이 나와야 하는 부분들이 tau가 커질수록 적게 나오는 모습을 볼 수 있다! 이는 tau 크기에 의해 고르게 분산되는, 뽑힌다는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 1.]], dtype=torch.float64) \n",
      "\n",
      "-1\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]], dtype=torch.float64) \n",
      "\n",
      "0\n",
      "tensor([[0., 0., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [0., 1., 1.]], dtype=torch.float64) \n",
      "\n",
      "1\n",
      "tensor([[0., 0., 1.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dim_num in range(-2,2):\n",
    "    print(dim_num)\n",
    "    result_hard = F.gumbel_softmax(test, hard=True, dim=dim_num)\n",
    "    print(result_hard, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
